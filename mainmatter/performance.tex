\chapter{Performance Analysis}\label{ch:performance}

\begin{keywords}
	me, vtd
\end{keywords}

The two following chapters are devoted to analysing the differences in performance and coding between the \gls{os} sockets and \software{0mq} sockets approach. The goal is to provide a consistent and valid set of data, that can be used to empower proper decision making, by weighting the pros and cons of each of the two approaches. As such measures are repeated multiple times in identical conditions, to obtain datasets with proper confidence levels.

Hardware, graphics settings, and scene are identical for all of the performance tests: the simulation is ran on the test 8-track, with \num{1} player, \num{2} \gls{ai} vehicles controlled by \gls{vtd}, a variable \gls{me} \gls{sendrate} (dependent on the \gls{framerate}), and a \SI{120}{\hertz} \gls{vtd} \gls{sendrate}. See \fref{tb:performance:hardware} for the hardware specifications.

\input{mainmatter/tables/performance/specifications}

\section{Framerate}\label{sc:performance:framerate}

\begin{definition}{framerate}
\end{definition}

\input{mainmatter/tables/performance/framerate}

\gls{me}'s \gls{framerate} is analysed, since the user's visual experience depends on it. Higher \glspl{framerate} are obviously better: the minimum acceptable \gls{framerate} is around \SI{30}{\fps}, with \SI{60}{\fps} or more being optimal on the more common \SI{60}{\hertz} monitors.

A game engine's \gls{framerate} depends on a large number of conditions, mainly graphics settings, how busy the scene is, underlying \gls{os} usage, hardware availability, and network issues. Generally speaking, better frames are achieved by:

\begin{itemize}
	\item \FONTbold{low graphics settings} --- Lower rendering resolution ($1080p$ or less), low quality/amount of \gls{fx}, basic shaders, no reflections, reduced \gls{pp} effects like \gls{dof}, blur, bloom, no \gls{aa} techniques.
	\item \FONTbold{sparse scenes} --- Scenes with less and/or more simple objects, like buildings, vehicles, or drivers.
	\item \FONTbold{idle \gls{os}} --- None or limited \gls{os} processes running in the background and utilizing the available hardware.
	\item \FONTbold{high-end hardware} --- High-clock \glspl{cpu}\footnote{Note that for game engines higher single core clock is usually more important than high thread counts.}, high-end gaming \glspl{gpu}, large (\SI{32}{\giga\byte} or more) amounts of fast \gls{ram}, \glspl{ssd} as opposed to \glspl{hdd}.
	\item \FONTbold{stable network} --- Low latency, no or minimal packet loss.
\end{itemize}

Measures were obtained with the benchmark feature of \software{Fraps} $3.5.99$. For each configuration (\gls{os} sockets, \software{0mq} sockets, unmodified \gls{me}) the measures were repeated \num{20} times (\SI{30}{\second} per run), obtaining three quantities:

\begin{itemize}
	\item \FONTbold{minimum} --- The minimum \gls{fps} reached during the \SI{30}{\second} run.
	\item \FONTbold{maximum} --- The maximum \gls{fps} reached during the \SI{30}{\second} run.
	\item \FONTbold{average} --- The average \gls{fps} of the \SI{30}{\second} run.
\end{itemize}

\FLOATnoindent Each of these $3\times3=9$ values was then averaged, and a \num{0,999} confidence interval calculated with a Student t's distribution. See \fref{ax:math:statistics} for some hints about statistical analysis.

\begin{image}
	{performance/framerate}{0.7}
	{framerate results}
	{im:performance:framerate}
	{}
\end{image} % TODO finish collecting FPS data

The tests show no appreciable difference in performance between \gls{os} sockets and \software{0mq} sockets. Maximum is \SI{143}{\fps} for both, with only a higher variability in the \software{0mq} case ($\pm$\SI{1,8}{\fps} as opposed to \SI{0,38}{\fps}). Minimum shows again a much greater variability for \software{0mq} ($\pm$\SI{6,7}{\fps}): with \SI{99,9}{\percent} confidence it can be assumed the minimums \gls{fps} are the same. Average \gls{framerate} displays more interesting results: \software{0mq} sockets are better performing by a very small margin, between \SI{0,10}{\fps} and \SI{7,90}{\fps} taking into account the confidence intervals.

In conclusion, \software{0mq} sockets do provide slightly better average performance than \gls{os} sockets; minimum and maximum \gls{fps} are not affected.

\section{Network}\label{sc:performance:network}

\begin{definition}{netcode}
\end{definition}

\input{mainmatter/tables/performance/network}

Data about network performance (the \gls{netcode}) is also collected. In a realistic working environment, \gls{me} and the \gls{ts} would most probably reside on a high-speed, dedicated wired connection. As such, performance measures like ping and packet loss are not collected: both depend heavily on the network infrastructure, so any results would not be easily transferable to real life situations.

Collection and basic analysis were done with the statistics feature of \software{Wireshark} $2.6.1$. As with \fref{sc:performance:framerate} for each configuration (\gls{os} sockets, \software{0mq} sockets, unmodified \gls{me}) the measures were repeated \num{20} times (\SI{30}{\second} per run),to obtain three quantities:

\begin{itemize}
	\item \FONTbold{packets per second} --- The average number of packets that each second insists on the channel (i.e.\ sent and received by both \gls{me} and \gls{vtd}).
	\item \FONTbold{load per second} --- The average amount of data that each second insists on the channel (i.e.\ sent and received by both \gls{me} and \gls{vtd}).
	\item \FONTbold{total load} --- The total amount of data exchanged between \gls{me} and \gls{vtd} in the whole duration of the run (\SI{30}{\second}).
\end{itemize}

\FLOATnoindent Each of these $3\times2=6$ values was then averaged, and a \num{0,999} confidence interval calculated with a Student t's distribution. See \fref{ax:math:statistics} for some hints about statistical analysis.

\begin{image}
	{performance/networkfrequency}{0.7}
	{network results (packet frequency)}
	{im:performance:networkfrequency}
	{}
\end{image}

\begin{image}
	{performance/networkdata}{0.7}
	{network results (data frequency)}
	{im:performance:networkdata}
	{}
\end{image}

\begin{image}
	{performance/networkload}{0.7}
	{network results (total data)}
	{im:performance:networkload}
	{}
\end{image}

As expected, total load and data frequency are identical, when taking into account the confidence intervals. This is because the bulk of the packets is formed by the payload: the data is relatively heavy due to the lack of encoding, hence the protocol's overhead bytes are mostly irrelevant. Packet frequency is instead completely different, due to the different protocols used. \gls{os} sockets with \gls{udp} only send about \SI{341}{\packets\per\second}, whilst \software{0mq} sockets with \gls{tcp} send twice that amount: \gls{tcp} (and in part \software{0mq}'s overhead) require small acknowledgement packets to be exchanged during the communication.\footnote{Note that all the results heavily depend on the sendrates: trivially, the higher, the more packets and data are exchanged.}

To conclude the network analysis, \gls{os} sockets only have an advantage in the packets frequency. Given the dedicated nature of the network infrastructure that is usually utilized in these projects, this advantage is hardly relevant: \software{0mq} thus shows it has no negative impact on network performance.

\section{Hardware}\label{sc:performance:hardware}

\input{mainmatter/tables/performance/hardware}

Hardware performance is also collected, to analyse which between \gls{os} or \software{0mq} sockets has the higher impact on \gls{cpu} and \gls{ram} usage. The same values were also taken with the unmodified \gls{me}, to obtain a baseline and see if the \gls{middleware} has any impact on resource usage.

Data was obtained with Windows' built-in performance monitor features. To ensure as best as possible independent subsequent runs, both \gls{me} and \gls{vtd} were restarted each time; additionally, instead of absolute \gls{ram} values relative deltas are used. Four quantities per \gls{middleware} configuration are measured:

\begin{itemize}
	\item \FONTbold{minimum \gls{cpu}} --- The minimum overall \gls{cpu} usage reached during the \SI{20}{\second} run.
	\item \FONTbold{maximum \gls{cpu}} --- The maximum overall \gls{cpu} usage reached during the \SI{20}{\second} run.
	\item \FONTbold{average \gls{cpu}} --- The average overall \gls{cpu} usage of the \SI{20}{\second} run.
	\item \FONTbold{\gls{ram} delta} --- The difference in used \gls{ram} between the moment the run starts and the moment it ends.
\end{itemize}

\FLOATnoindent Each of these $4\times3=12$ values was then averaged, and a \num{0,950} confidence interval calculated with a Student t's distribution. See \fref{ax:math:statistics} for some hints about statistical analysis.

\begin{image}
	{performance/hardwarecpu}{0.7}
	{hardware results (\gls{cpu} usage)}
	{im:performance:hardwarecpu}
	{}
\end{image}

\begin{image}
	{performance/hardwareram}{0.7}
	{hardware results (\gls{ram} usage)}
	{im:performance:hardwareram}
	{}
\end{image}

Again, taking into account the confidence intervals, the \gls{middleware} with the \gls{os} sockets implementation has no impact on four out of five quantities: only the average \gls{cpu} usage shows a minimal impact. Interestingly, the \software{0mq} sockets implementation has a relevant effect on \gls{cpu} usage: between a \SI{6,5}{\percent} and \SI{12}{\percent} increase in minimum load; between a \SI{4,8}{\percent} and \SI{19}{\percent} increase in maximum load; between a \SI{5,4}{\percent} and \SI{13}{\percent} increase in average load. This is probably due to the additional overhead introduced by \software{0mq}'s high level functions, and the additional packets required by the \gls{tcp} protocol and \software{0mq}'s \FONTsmallcaps{pub/sub} model. \gls{ram} delta variability is too high to appreciate any changes between the implementations.

When hardware usage is concerned, \gls{os} sockets have a significant advantage over \software{0mq}, with lower all-around hardware usage. It must be noted though that the higher hardware usage does not translate in lower \gls{me} performance (see \fref{sc:performance:framerate}): hypothetically, this should hold true until \gls{me}'e performance is not \gls{cpu} bound.

\section{Proxy}\label{sc:performance:proxy}

\input{mainmatter/tables/performance/proxy}

To ensure the \gls{proxy} has no noticeable ill effects in testing \software{0mq} sockets, the amount of time a packet spends in the \gls{proxy} has been measured. These are more precisely three measures: the minimum, maximum, and average time passed between the moment a packet is received by the \gls{proxy}, and the moment that same packet is sent. \planguage{c}'s \code{clock()} function is used to calculate this time delta: a basic \code{Evaluator} namespace was coded for this purpose. See \fref{ax:code:proxy} to see where its functions are used in the proxy code (lines \num{167} and \num{203}).

\begin{codelist}{Evaluator.cpp}{10}
namespace ZMQBridge {

clock_t Evaluator::m_timeIn;
clock_t Evaluator::m_timeOut;

std::vector<float> Evaluator::m_deltas;

std::ofstream Evaluator::m_file;

void Evaluator::start() {
    m_timeIn = clock();
}

void Evaluator::end() {
    m_timeOut = clock();
}

void Evaluator::compute_delta() {
    m_deltas.push_back(((float) (m_timeOut - m_timeIn)) / CLOCKS_PER_SEC);
}

void Evaluator::log_data() {
    m_file.open("perf.csv", std::ios::out | std::ios::app);

    if (m_file.is_open()) {
        m_file << "delta\n"
               << std::fixed;

        for (std::vector<float>::const_iterator iter = m_deltas.begin(); iter != m_deltas.end(); iter++) {
            m_file << *iter << "\n";
        }
    }

    m_file.close();
}

}
\end{codelist}

\FLOATnoindent Each of these \num{3} values was then averaged, and a \num{0,950} confidence interval calculated with a Student t's distribution. See \fref{ax:math:statistics} for some hints about statistical analysis.

\begin{image}
	{performance/proxy}{0.7}
	{proxy results}
	{im:performance:proxy}
	{}
\end{image}

Even at its maximum value, the time a packet spends in the proxy is only between \SI{0,834}{\milli\second} and \SI{1,614}{\milli\second} (keeping into account the confidence range around the \SI{1,224}{\milli\second} value). Average is much smaller still (\SI{0,136}{\milli\second} $\pm$ \SI{0,103}{\milli\second}). To understand what this delay means to the user on \gls{me}'s side, the impact the proxy has on correct frame pacing is analysed. Frame pacing is the time that passes between two consecutive frames, and is generally tied to \gls{framerate}: that is, assuming frames are spaced evenly (which is often not the case in real-life engines), frame spacing may be calculated by dividing a second by the \gls{fps}.

\begin{image}
	{performance/proxyspacing}{0.7}
	{\gls{framerate} pacing}
	{im:performance:proxyspacing}
	{}
\end{image}

In \fref{im:performance:proxylandscape} the \gls{framerate} and dwell time are plotted (respectively the \code{x} and \code{y} axis), and the resulting coloured landscape represents the frame delay (in percentage of a single frame) introduced by the proxy. The lines represent increments of \SI{5}{\percent}, and the more the colour is saturated the higher the percentage. Clearly, higher percentages (more delay) happen with high proxy dwell times, and high \gls{framerate} (i.e.\ low frame times); also, higher proxy dwell times have a broader variation in frame delay along the possible \gls{framerate}, \SI{0}{\percent} to \SI{40}{\percent} as opposed to \SI{0}{\percent} to \SI{20}{\percent} in the lower dwell time. The dwell times analysed are those within the maximum dwell time confidence interval.

\begin{image}
	{performance/proxylandscape}{1.0}
	{frame delay landscape}
	{im:performance:proxylandscape}
	{}
\end{image}

"Slicing" this landscape through a specific \gls{framerate} can more clearly show the frame delay behaviour across the proxy dwell time interval: \fref{im:performance:proxyslice} shows such a slice on the average \gls{framerate} (see \fref{sc:performance:framerate}). The relationship is linear, but with the real average framerate found during testing even at maximum dwell times the frame delay is of only \SI{20}{\percent}.

\begin{image}
	{performance/proxyslice}{0.7}
	{average \gls{framerate} slice}
	{im:performance:proxyslice}
	{}
\end{image}

In conclusion, the proxy has little to no impact on the system, up to \SI{20}{\percent} of a frame, so the tests and analyses made on \software{0mq} sockets are not invalidated by the inability to properly modify \gls{vtd} to integrate \software{0mq}.

\section{Results}\label{sc:performance:results}

What follows is a recap of the test results. The main focus is \gls{os} sockets versus \software{0mq} sockets, and the advantages and drawbacks of each solution: the results are summarized in \fref{tb:performance:results}.

\input{mainmatter/tables/performance/results}

The advantage in rate (i.e. \si{\packets\per\second}) is intrinsic in the \gls{tcp} protocol available in \software{0mq}, but has a limited impact in a real-life enterprise environment, where network connections are mostly fault-proof and high-speed. The framerate average is very lightly skewed in \software{0mq}'s favour, in any case not pushing the framerate above any relevant value (like \SI{120}{\fps} or \SI{144}{\fps} for higher refresh rate monitors). The real difference is in hardware usage, although with no actual impact on performance. Provided the system running \gls{me} is not \gls{cpu} bounded, the advantages of \gls{os} sockets are numerical only: framerate is not affected.

All in all, it is fair to deem the performance to be largely equal between \gls{os} sockets and \software{0mq} sockets: the advantages one has on the other are either small, or not impactful in a real life scenario. Performance not being the choosing point, code ease and complications will be analysed next.

\section{Recap}\label{sc:performance:recap}

This chapter has been devoted to the analysis of the performance of a number of elements in the system. It was found that neither \gls{os} sockets or \software{0mq} sockets have a noticeable impact in \gls{me} in \fref{sc:performance:framerate}, although they show some key differences mainly in networking and \gls{cpu} usage (\fref{sc:performance:network} and \fref{sc:performance:hardware}). The proxy allowing the \software{0mq} solution to work was also shown to be unimpactful on the rest of the performance (\fref{sc:performance:proxy}).
